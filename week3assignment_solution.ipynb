{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 3: ASSIGNMENT\n",
    "Data Analysis and Visulization:\n",
    "#### What is the Analysis?   A detailed examination of anything complex in order to understand its nature or to determine its essential features\n",
    "\n",
    "Data Analysis is the process of cleaning, transforming, visualizing, and analyzing the data to gain valuable insights to make more effective business decisions is known as Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Advanced Pandas\n",
    "1-https://www.javatpoint.com/pandas-cheat-sheet,\n",
    "2-https://python.plainenglish.io/a-comprehensive-guide-to-pandas-cheat-sheet-for-data-science-enthusiasts-b6f131ab5284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1-Working with Multi-index DataFrames\n",
    "Multi-index allows you to select more than one row and column in your index.Mean we can combine/add/append multiple arrays,tuples,frames with different values as columns or rows to make one dataset.\n",
    "We can use various methods of multi-index such as MultiIndex.from_arrays(), MultiIndex.from_tuples(), MultiIndex.from_product(), MultiIndex.from_frame, etc., which helps us to create multiple indexes from arrays, tuples, DataFrame,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Annual Salary(L.P.A)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saikat</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shrestha</td>\n",
       "      <td>System Engineer</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sandi</td>\n",
       "      <td>Footballer</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abinash</td>\n",
       "      <td>Singer</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                Jobs  Annual Salary(L.P.A)\n",
       "0    Saikat  Software Developer                  12.4\n",
       "1  Shrestha     System Engineer                   5.6\n",
       "2     Sandi          Footballer                   9.3\n",
       "3   Abinash              Singer                  10.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating data\n",
    "Information = {'name': [\"Saikat\", \"Shrestha\", \"Sandi\", \"Abinash\"],\n",
    "               \n",
    "               'Jobs': [\"Software Developer\", \"System Engineer\",\n",
    "                        \"Footballer\", \"Singer\"],\n",
    "               \n",
    "               'Annual Salary(L.P.A)': [12.4, 5.6, 9.3, 10]}\n",
    "\n",
    "# Dataframing the whole data\n",
    "df = pd.DataFrame(Information)\n",
    "\n",
    "# Showing the above data\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the multi-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(  'Saikat', 'Software Developer', 12.4),\n",
       "            ('Shrestha',    'System Engineer',  5.6),\n",
       "            (   'Sandi',         'Footballer',  9.3),\n",
       "            ( 'Abinash',             'Singer', 10.0)],\n",
       "           names=['name', 'Jobs', 'Annual Salary(L.P.A)'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating multiple indexes from the dataframe\n",
    "pd.MultiIndex.from_frame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### here we are appending the columns by creating the dictionary to the dataframe already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(0, 'Peaky blinders', 4.5),\n",
      "            (1,       'Sherlock', 5.0),\n",
      "            (2,      'The crown', 3.9),\n",
      "            (3,  'Queens Gambit', 4.2),\n",
      "            (4,        'Friends', 5.0)],\n",
      "           names=[None, 'series', 'Ratings'])\n"
     ]
    }
   ],
   "source": [
    "# making data for dataframing\n",
    "data = {\n",
    "    'name': [\"Saikat\", \"Shrestha\", \"Sandi\", \"Abinash\",\"Don\"],\n",
    "    'series': ['Peaky blinders', 'Sherlock', 'The crown',\n",
    "               'Queens Gambit', 'Friends'],\n",
    "    \n",
    "    'Ratings': [4.5, 5, 3.9, 4.2, 5],\n",
    "    \n",
    "    'Date': [2013, 2010, 2016, 2020, 1994]\n",
    "}\n",
    "\n",
    "# Dataframing the whole data created\n",
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "# setting first and the second name\n",
    "# as index column\n",
    "df2.set_index([\"series\", \"Ratings\"], inplace=True,     #as we have 3 things rename,replace,append/add either rows or columns\n",
    "             append=True, drop=False)\n",
    "# display the dataframe\n",
    "print(df2.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Merging On \n",
    "dataframes based on columns,dataframes with same columns,on multiple columns,on different columns,series into dataframe,multiple dataframes\n",
    "four types we can do merging: left(take left df and common of right),right(take the right df and common of left),inner(on common column base),outer(take all the columns removing the common columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>series</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Date</th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Annual Salary(L.P.A)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abinash</td>\n",
       "      <td>Queens Gambit</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2020</td>\n",
       "      <td>Singer</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Don</td>\n",
       "      <td>Friends</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saikat</td>\n",
       "      <td>Peaky blinders</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2013</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sandi</td>\n",
       "      <td>The crown</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2016</td>\n",
       "      <td>Footballer</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shrestha</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>System Engineer</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name          series  Ratings  Date                Jobs  \\\n",
       "0   Abinash   Queens Gambit      4.2  2020              Singer   \n",
       "1       Don         Friends      5.0  1994                 NaN   \n",
       "2    Saikat  Peaky blinders      4.5  2013  Software Developer   \n",
       "3     Sandi       The crown      3.9  2016          Footballer   \n",
       "4  Shrestha        Sherlock      5.0  2010     System Engineer   \n",
       "\n",
       "   Annual Salary(L.P.A)  \n",
       "0                  10.0  \n",
       "1                   NaN  \n",
       "2                  12.4  \n",
       "3                   9.3  \n",
       "4                   5.6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged=df2.merge(df[[\"name\",\"Jobs\",\"Annual Salary(L.P.A)\"]],on=\"name\",how=\"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 joining\n",
    "difference between the three is that join combines DataFrames based on index while merge combines with both indexes or columns.merge is a useful function for combining Data Frames on specific columns. Join is a useful function for combining along indexes or key columns. Lastly, concat is most useful for combining along either axis or for multi-indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Annual Salary(L.P.A)</th>\n",
       "      <th>series</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abinash</th>\n",
       "      <td>Singer</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Queens Gambit</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Don</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friends</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saikat</th>\n",
       "      <td>Software Developer</td>\n",
       "      <td>12.4</td>\n",
       "      <td>Peaky blinders</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sandi</th>\n",
       "      <td>Footballer</td>\n",
       "      <td>9.3</td>\n",
       "      <td>The crown</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shrestha</th>\n",
       "      <td>System Engineer</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Jobs  Annual Salary(L.P.A)          series  Ratings  \\\n",
       "name                                                                          \n",
       "Abinash               Singer                  10.0   Queens Gambit      4.2   \n",
       "Don                      NaN                   NaN         Friends      5.0   \n",
       "Saikat    Software Developer                  12.4  Peaky blinders      4.5   \n",
       "Sandi             Footballer                   9.3       The crown      3.9   \n",
       "Shrestha     System Engineer                   5.6        Sherlock      5.0   \n",
       "\n",
       "          Date  \n",
       "name            \n",
       "Abinash   2020  \n",
       "Don       1994  \n",
       "Saikat    2013  \n",
       "Sandi     2016  \n",
       "Shrestha  2010  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined=df.set_index('name').join(df2.set_index('name'), how='outer')\n",
    "\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Concatenation on datasets\n",
    "combining the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Annual Salary(L.P.A)</th>\n",
       "      <th>name</th>\n",
       "      <th>series</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saikat</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>12.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shrestha</td>\n",
       "      <td>System Engineer</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sandi</td>\n",
       "      <td>Footballer</td>\n",
       "      <td>9.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abinash</td>\n",
       "      <td>Singer</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, Peaky blinders, 4.5)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saikat</td>\n",
       "      <td>Peaky blinders</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, Sherlock, 5.0)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shrestha</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, The crown, 3.9)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sandi</td>\n",
       "      <td>The crown</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, Queens Gambit, 4.2)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abinash</td>\n",
       "      <td>Queens Gambit</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, Friends, 5.0)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Don</td>\n",
       "      <td>Friends</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name                Jobs  Annual Salary(L.P.A)  \\\n",
       "0                           Saikat  Software Developer                  12.4   \n",
       "1                         Shrestha     System Engineer                   5.6   \n",
       "2                            Sandi          Footballer                   9.3   \n",
       "3                          Abinash              Singer                  10.0   \n",
       "(0, Peaky blinders, 4.5)       NaN                 NaN                   NaN   \n",
       "(1, Sherlock, 5.0)             NaN                 NaN                   NaN   \n",
       "(2, The crown, 3.9)            NaN                 NaN                   NaN   \n",
       "(3, Queens Gambit, 4.2)        NaN                 NaN                   NaN   \n",
       "(4, Friends, 5.0)              NaN                 NaN                   NaN   \n",
       "\n",
       "                              name          series  Ratings    Date  \n",
       "0                              NaN             NaN      NaN     NaN  \n",
       "1                              NaN             NaN      NaN     NaN  \n",
       "2                              NaN             NaN      NaN     NaN  \n",
       "3                              NaN             NaN      NaN     NaN  \n",
       "(0, Peaky blinders, 4.5)    Saikat  Peaky blinders      4.5  2013.0  \n",
       "(1, Sherlock, 5.0)        Shrestha        Sherlock      5.0  2010.0  \n",
       "(2, The crown, 3.9)          Sandi       The crown      3.9  2016.0  \n",
       "(3, Queens Gambit, 4.2)    Abinash   Queens Gambit      4.2  2020.0  \n",
       "(4, Friends, 5.0)              Don         Friends      5.0  1994.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated=pd.concat([df,df2],axis=1)\n",
    "df_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"E:\\\\Programming\\\\internship\\\\week3assignment\\\\merch_sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Handling missing data and cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data count per column:\n",
      "name                    1\n",
      "Jobs                    1\n",
      "Annual Salary(L.P.A)    1\n",
      "dtype: int64\n",
      "\n",
      "Cleaned DataFrame:\n",
      "       name                Jobs  Salary\n",
      "0    Saikat  Software Developer  12.400\n",
      "1  Shrestha     System Engineer   5.600\n",
      "2   Unknown          Footballer   9.300\n",
      "3   Abinash       Not Specified   9.325\n",
      "4       Don              Singer  10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Saikat', 'Shrestha', np.nan, 'Abinash', 'Don'],\n",
    "    'Jobs': ['Software Developer', 'System Engineer', 'Footballer', np.nan, 'Singer'],\n",
    "    'Annual Salary(L.P.A)': [12.4, 5.6, 9.3, np.nan, 10]\n",
    "})\n",
    "\n",
    "# 1. Identify missing data\n",
    "print(\"Missing data count per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Fill missing values\n",
    "df['name'] = df['name'].fillna('Unknown')  # Fill 'name' with 'Unknown'\n",
    "df['Jobs'] = df['Jobs'].fillna('Not Specified')  # Fill 'Jobs' with 'Not Specified'\n",
    "df['Annual Salary(L.P.A)'] = df['Annual Salary(L.P.A)'].fillna(df['Annual Salary(L.P.A)'].mean())  # Fill salary with mean\n",
    "\n",
    "# 3. Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 4. Handle outliers (capping in this case)\n",
    "df['Annual Salary(L.P.A)'] = df['Annual Salary(L.P.A)'].clip(lower=5, upper=15)\n",
    "\n",
    "# 5. Rename columns for clarity\n",
    "df.rename(columns={'Annual Salary(L.P.A)': 'Salary'}, inplace=True)\n",
    "\n",
    "# Display cleaned dataframe\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6- Grouping and Aggregate using custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name                Jobs  Annual Salary(L.P.A)  Age\n",
      "0    Saikat  Software Developer                  12.4   28\n",
      "1  Shrestha     System Engineer                   5.6   30\n",
      "2     Sandi          Footballer                   9.3   25\n",
      "3   Abinash              Singer                  10.0   22\n",
      "4       Don          Footballer                   7.5   27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'name': ['Saikat', 'Shrestha', 'Sandi', 'Abinash', 'Don'],\n",
    "    'Jobs': ['Software Developer', 'System Engineer', 'Footballer', 'Singer', 'Footballer'],\n",
    "    'Annual Salary(L.P.A)': [12.4, 5.6, 9.3, 10.0, 7.5],\n",
    "    'Age': [28, 30, 25, 22, 27]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000024D9B16E660>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped=df.groupby('Jobs')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jobs\n",
       "Footballer             8.4\n",
       "Singer                10.0\n",
       "Software Developer    12.4\n",
       "System Engineer        5.6\n",
       "Name: Annual Salary(L.P.A), dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated=grouped['Annual Salary(L.P.A)'].mean() #similar for sum,min,max\n",
    "aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jobs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Footballer</th>\n",
       "      <td>7.5</td>\n",
       "      <td>9.3</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singer</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software Developer</th>\n",
       "      <td>12.4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System Engineer</th>\n",
       "      <td>5.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     min   max   sum\n",
       "Jobs                                \n",
       "Footballer           7.5   9.3  16.8\n",
       "Singer              10.0  10.0  10.0\n",
       "Software Developer  12.4  12.4  12.4\n",
       "System Engineer      5.6   5.6   5.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated=grouped['Annual Salary(L.P.A)'].agg(['min','max','sum'])\n",
    "aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Numpy Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues & Eigenvector\n",
    "**Scalar:** 0-dimensional, just a single number.Example: 5, 3.14, -42<br>\n",
    "**Vector:** 1-dimensional, an array of numbers. Example: [1, 2, 3], [0, 3.5, 6, 9]<br>\n",
    "**Matrix:** 2-dimensional, an array of numbers arranged in rows and columns.Example: [[1, 2], [3, 4]], [[5, 6, 7], [8, 9, 10]] <br>\n",
    "Here basically we have matrixlets say A=[1 2 3] and vector which is also a matrix X=[4 5 6 ],& when we compute them by performing the dot product ,at the end we have some value which is either 0 or some number that's what we can say our   \n",
    "Compute the product AX\n",
    " for\n",
    "AX=kX\n",
    "  where  k\n",
    "  is some scalar,When this equation holds for some  X\n",
    "  and  k\n",
    " , we call the scalar  k\n",
    "  an eigenvalue of  A\n",
    " . We often use the special symbol  Œª\n",
    "  instead of  k\n",
    "  when referring to eigenvalues. In Example  7.1.1\n",
    " , the values  10\n",
    "  and  0\n",
    "  are eigenvalues for the matrix  A\n",
    "  and we can label these as  Œª1=10\n",
    "  and  Œª2=0\n",
    " \n",
    "When  AX=ŒªX\n",
    "  for some  X‚â†0\n",
    " , we call such an  X\n",
    "  an eigenvector of the matrix  A\n",
    " . The eigenvectors of  A\n",
    "  are associated to an eigenvalue. Hence, if  Œª1\n",
    "  is an eigenvalue of  A\n",
    "  and  AX=Œª1X\n",
    " , we can label this eigenvector as  X1\n",
    " . Note again that in order to be an eigenvector,  X\n",
    "  must be nonzero.\n",
    "Let‚Äôs say you have a matrix \n",
    "ùê¥\n",
    "A representing some transformation in space. If you multiply this matrix by a vector \n",
    "ùëã\n",
    "X, it will generally rotate and scale \n",
    "ùëã\n",
    "X. But if \n",
    "ùëã\n",
    "X is an eigenvector of \n",
    "ùê¥\n",
    "A, then multiplying \n",
    "ùê¥\n",
    "A by \n",
    "ùëã\n",
    "X will only scale \n",
    "ùëã\n",
    "X by a factor \n",
    "ùúÜ\n",
    "Œª, without changing its direction. In other words:\n",
    "\n",
    "ùê¥\n",
    "ùëã\n",
    "=\n",
    "ùúÜ\n",
    "ùëã\n",
    "AX=ŒªX\n",
    "So, the eigenvalue \n",
    "ùúÜ\n",
    "Œª represents how much the vector is stretched or compressed, and the eigenvector \n",
    "ùëã\n",
    "X represents the direction that stays unchanged under the transformation by matrix \n",
    "ùê¥\n",
    "This equation means that when we apply the matrix \n",
    "ùê¥\n",
    "A to the vector \n",
    "ùëã\n",
    "X, the vector \n",
    "ùëã\n",
    "X only gets scaled by \n",
    "ùúÜ\n",
    "Œª (but not rotated or changed direction). The eigenvector \n",
    "ùëã\n",
    "X remains in the same direction after being transformed by \n",
    "ùê¥\n",
    "A, but its length is scaled by the corresponding eigenvalue \n",
    "ùúÜ\n",
    "Œª.\n",
    "  for furthere understanding you can look at here this link    https://byjus.com/maths/eigen-values/\n",
    "  ##### Why/Purpose of eigenvalues and vector\n",
    "**1. Simplifying Matrix Operations:**\n",
    "Eigenvalues and eigenvectors allow us to diagonalize a matrix (if possible), which simplifies matrix operations. When a matrix is diagonalized, it is represented as a product of its eigenvectors and eigenvalues, making many matrix calculations, like exponentiation, much easier.\n",
    "\n",
    "Example: In physics or in systems analysis, diagonalization helps reduce complex systems of linear equations to simpler forms.\n",
    "\n",
    "**2. Data Reduction (PCA in Machine Learning):**\n",
    "Eigenvalues and eigenvectors are used in Principal Component Analysis (PCA), a common technique in machine learning and data science for dimensionality reduction. PCA helps identify the \"principal components\" of the data, which are the directions (eigenvectors) that explain the most variance in the data. The eigenvalues tell you how much variance each principal component explains.\n",
    "\n",
    "***Why it matters:*** In large datasets, many features (dimensions) might be redundant. PCA reduces the dimensions of the data while retaining most of the information, which makes the data easier to work with (e.g., faster training in machine learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [3. 2.]\n",
      "Eigenvectors:\n",
      " [[0.89442719 0.70710678]\n",
      " [0.4472136  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a square matrix\n",
    "A = np.array([[4, -2], \n",
    "              [1,  1]])\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition (SVD)\n",
    "The Singular Value Decomposition of a matrix is a factorization of the matrix into three matrices. Thus, the singular value decomposition of matrix A can be expressed in terms of the factorization of A into the product of three matrices as A = UDVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\n",
      " [[-0.40455358 -0.9145143 ]\n",
      " [-0.9145143   0.40455358]]\n",
      "S (singular values):\n",
      " [5.4649857  0.36596619]\n",
      "Vt (transposed V):\n",
      " [[-0.57604844 -0.81741556]\n",
      " [ 0.81741556 -0.57604844]]\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix\n",
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "\n",
    "# Perform Singular Value Decomposition\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(\"U:\\n\", U)\n",
    "print(\"S (singular values):\\n\", S)\n",
    "print(\"Vt (transposed V):\\n\", Vt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Operations\n",
    "NumPy allows you to perform element-wise operations on arrays or matrices, making computations faster and more efficient than traditional Python loops. This is called vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Perform element-wise addition\n",
    "C = A + B\n",
    "print(\"Element-wise addition:\\n\", C)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "D = A * B\n",
    "print(\"Element-wise multiplication:\\n\", D)\n",
    "\n",
    "# Perform matrix multiplication (dot product)\n",
    "E = np.dot(A, B)\n",
    "print(\"Matrix multiplication (dot product):\\n\", E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Optimization (Avoid Loops with NumPy)\n",
    "One of the main advantages of NumPy is performance. NumPy operations are optimized and written in C, which makes them much faster than traditional Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a large matrix\n",
    "A = np.random.rand(1000, 1000)\n",
    "\n",
    "# Using a loop to square every element (slow)\n",
    "result_loop = np.zeros_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        result_loop[i, j] = A[i, j] ** 2\n",
    "\n",
    "# Using a vectorized operation to square every element (fast)\n",
    "result_vectorized = A ** 2\n",
    "\n",
    "print(\"Result (loop) and (vectorized) are the same:\", np.allclose(result_loop, result_vectorized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "Broadcasting is a powerful feature in NumPy that allows you to perform arithmetic operations on arrays of different shapes. NumPy automatically expands the smaller array to match the shape of the larger one in a process known as broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting result:\n",
      " [[2 4]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "# Define a 2D matrix and a 1D vector\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([1, 2])\n",
    "\n",
    "# Broadcasting allows adding a 1D vector to a 2D matrix\n",
    "C = A + B\n",
    "print(\"Broadcasting result:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping\n",
    "Reshaping allows you to change the shape of a NumPy array without changing its data. This is useful when you need to prepare data for matrix operations or machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 1D array\n",
    "A = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Reshape it into a 2x3 matrix\n",
    "B = A.reshape(2, 3)\n",
    "print(\"Reshaped matrix (2x3):\\n\", B)\n",
    "\n",
    "# Flatten the matrix back into a 1D array\n",
    "C = B.flatten()\n",
    "print(\"Flattened array:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Operations (Determinants, Inverses, etc.)\n",
    "You can also perform other advanced matrix operations such as computing the determinant or the inverse of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determinant\n",
    "its basically done to check the matrix either it is feasible or not for furthere operations like inversing etc.\n",
    "its a scalar value after determinant it means it can be used otherwise not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of A: -2.0\n"
     ]
    }
   ],
   "source": [
    "# Define a square matrix\n",
    "A = np.array([[4, 2], [3, 1]])\n",
    "\n",
    "# Calculate the determinant\n",
    "det = np.linalg.det(A)\n",
    "print(\"Determinant of A:\", det)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverse\n",
    "Since we can‚Äôt do division with matrix so we use inverse of a matrix for division.\n",
    "\n",
    "For example: If you have to divide 10 pens to 2 people you can simply do 10/2=5 or you can find the inverse of 2 i.e 0.5 and multiply it with 10 which gives (10*0.5 =5)\n",
    "Similarly if we have to calculate value of matrix X in the equation X A= B\n",
    "\n",
    "Then, X=B/A (but since we can‚Äôt do division in matrix we can multiply B with inverse of A).\n",
    "\n",
    "So this can be written as X A (A)^-1 = B (A)^-1 (multiplying both sides with the inverse of A).\n",
    "\n",
    "The above will give X*I = B(A)^-1 (I=Identity matrix,we can replace it with it‚Äôs magnitude which is 1 as A(A)^-1 = I).\n",
    "As 2.1/2 =1 similarly A.A-1=1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the inverse of a matrix\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"Inverse of A:\\n\", A_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Data Visualization with matplotlib and seaborn\n",
    "**Seaborn vs Matplotlib**\n",
    "**Matplotlib** is a low-level plotting library that provides a high degree of control over individual elements. Even for basic functionalities, it requires more code.\n",
    "\n",
    "Whereas **seaborn** is a high level library for visualization and requires less coding compared to matplotli.\n",
    "\n",
    "**Matplotlib** lets users customize the appearances of plots, including color and styles.\n",
    "\n",
    "**Seaborn** has in-built themes and color palettes making it easier for users to create visually appealing plots.\n",
    "\n",
    "**Matplotlib** can work with pandas but users may need to manipulate data for certain type of plots.\n",
    "\n",
    "**Seaborn** is very much flexible with pandas and it doesn‚Äôt require as much manipulation as matplotlib.\n",
    "\n",
    "**Annotations** allow users to label data points and indicate trends or add descriptions to different parts of a plot.\n",
    "\n",
    "**Both seaborn and matplotlib have style themes.**\n",
    "\n",
    "There are **five themes** in seaborn:\n",
    "`white,\n",
    "dark,\n",
    "whitegrid,\n",
    "darkgrid,\n",
    "ticks`\n",
    "We will use style theme from matplotlib. Matplotlib gives 26 styles which you can be seen with the `plt.style.available` method.\n",
    "**Output:**\n",
    "`['Solarize_Light2',\n",
    " '_classic_test_patch',\n",
    " 'bmh',\n",
    " 'classic',\n",
    " 'dark_background',\n",
    " 'fast',\n",
    " 'fivethirtyeight',\n",
    " 'ggplot',\n",
    " 'grayscale',\n",
    " 'seaborn',\n",
    " 'seaborn-bright',\n",
    " 'seaborn-colorblind',\n",
    " 'seaborn-dark',\n",
    " 'seaborn-dark-palette',\n",
    " 'seaborn-darkgrid',\n",
    " 'seaborn-deep',\n",
    " 'seaborn-muted',\n",
    " 'seaborn-notebook',\n",
    " 'seaborn-paper',\n",
    " 'seaborn-pastel',\n",
    " 'seaborn-poster',\n",
    " 'seaborn-talk',\n",
    " 'seaborn-ticks',\n",
    " 'seaborn-white',\n",
    " 'seaborn-whitegrid',\n",
    " 'tableau-colorblind10']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Visualization?\n",
    "Visualization refers to techniques used to communicate both abstract and concrete,behaviour,information, ideas by creating images, diagrams, or animations of objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1- Data Visualization Explanation\n",
    "Data visualization uses graphical representations like graphs, charts, and maps to simplify complex data, making it easier to understand and analyze. Visuals allow data scientists to summarize thousands of rows and columns of complex data and put it in an understandable and accessible format. It helps identify patterns, correlations, and outliers, providing a more effective analysis than tables or descriptive statistics. Data visualization is crucial in decision-making, enabling data analysts, scientists, and engineers to communicate insights to non-technical stakeholders and inform actions, such as A/B testing or addressing bias in models like ChatGPT.  \n",
    "Link to understand more about the data visualization: [What is Data Visualization?](https://www.couchbase.com/blog/what-is-data-analysis/)\n",
    "\n",
    "#### A comprehensive breakdown of Data Analysis and Data Visualization concepts, organized into a structured tree format to clarify the relationship between different elements:\n",
    "\n",
    "#### 1. Data Types\n",
    "- **Quantitative Data**: Numerical values, used for statistical analysis.\n",
    "- **Qualitative Data**: Descriptive data, typically non-numerical and used for interpretive analysis.\n",
    "\n",
    "#### 2. Types of Data Analysis\n",
    "- **Univariate Analysis**: Univariate Analysis for Continuous Variables and Categorical Variables.\n",
    "- **Bivariate Analysis**: Bivariate Analysis for Continuous Variable vs Continuous Variable, Categorical Variable vs Categorical Variable.\n",
    "- **Multivariate Analysis**: Multivariate Analysis for Numerical-Numerical,Numerical-Categorical Variables.\n",
    "\n",
    "#### 3. Data Analysis Techniques\n",
    "- **Descriptive Analysis**: Summarizes main features (e.g., mean, median, mode).  \n",
    "  **Tools**: Histograms, Bar Charts.  \n",
    "- **Diagnostic Analysis**: Investigates causes and patterns (e.g., regression, correlation).  \n",
    "  **Tools**: Scatter plots, Heatmaps.  \n",
    "- **Predictive Analysis**: Forecasts future outcomes using historical data.  \n",
    "  **Tools**: Line plots, Forecasting models.  \n",
    "- **Prescriptive Analysis**: Provides recommendations based on predictive insights.  \n",
    "  **Tools**: Optimization models, Decision trees.  \n",
    "\n",
    "#### 4. Types of Data Analysis Methods\n",
    "- **Statistical Analysis**: Applies statistical techniques (e.g., regression, hypothesis testing).  \n",
    "  **Tools**: R, Python (scikit-learn, NumPy).  \n",
    "- **Data Mining**: Discovers patterns from large datasets.  \n",
    "  **Tools**: Clustering, Classification algorithms.  \n",
    "- **Text Mining**: Analyzes unstructured textual data.  \n",
    "  **Tools**: Natural Language Processing (NLP), sentiment analysis.  \n",
    "- **Time Series Analysis**: Focuses on data points collected over time.  \n",
    "  **Tools**: Forecasting, Decomposition.\n",
    "\n",
    "#### 5. Key Data Visualization Techniques\n",
    "- **Distribution**: Analyzing how data points are spread.  \n",
    "  **Visualizations**: Histograms, Boxplots.  \n",
    "- **Correlation**: Identifying relationships between variables.  \n",
    "  **Visualizations**: Scatter plots, Heatmaps.  \n",
    "- **Ranking**: Comparing quantities in order.  \n",
    "  **Visualizations**: Bar charts, Dot plots.  \n",
    "- **Part-of-Whole**: Showing proportions of a total.  \n",
    "  **Visualizations**: Pie charts, Stacked Bar charts.  \n",
    "- **Evolution**: Analyzing changes over time.  \n",
    "  **Visualizations**: Line charts, Area charts.  \n",
    "- **Map**: Geospatial data visualization.  \n",
    "  **Visualizations**: Choropleth maps, Heat maps.  \n",
    "- **Networks**: Analyzing connections or relationships.  \n",
    "  **Visualizations**: Network graphs, Force-directed graphs.\n",
    "\n",
    "#### 6. Deciding the Right Technique\n",
    "**Question to Ask**:  \n",
    "- How many variables are you analyzing?  \n",
    "  - **Univariate Analysis**: Analyzing a single variable to understand its distribution, central tendency, and variability.  \n",
    "  - **Bivariate Analysis**: Analyzing the relationship between two variables, often looking for correlations or associations.  \n",
    "  - **Multivariate Analysis**: Analyzing the relationships between three or more variables simultaneously.\n",
    "\n",
    "**What do you want to analyze?**  \n",
    "- Use suitable techniques for distribution, correlation, ranking, etc.  \n",
    "- Choose appropriate visualizations depending on analysis goals.\n",
    "\n",
    "#### 7. Data Analysis Process\n",
    "- **Define Objective**: Understand what you want to achieve.\n",
    "- **Prepare and Explore Data**: Clean data, check for outliers.\n",
    "- **Apply Analysis Techniques**: Choose methods (statistical, machine learning, etc.).\n",
    "- **Interpret Results**: Derive meaningful insights.\n",
    "- **Communicate Findings**: Use visualizations and reports to convey results effectively.\n",
    "\n",
    "#### 8. Common Data Analysis Tools\n",
    "- **Spreadsheet Software**: Basic analysis (Excel, Google Sheets).\n",
    "- **Business Intelligence Platforms**: Interactive dashboards (Power BI, Tableau).\n",
    "- **Programming Languages**: Advanced analysis (Python, R).\n",
    "- **Cloud-Based Platforms**: Scalable environments for large datasets (Google Cloud, AWS).\n",
    "- **Text Analytics Tools**: Analyzing unstructured data (NLP libraries, sentiment analysis tools).\n",
    "\n",
    "#### 9. Importance of Data Analysis\n",
    "- **Decision Making**: Provides insights for informed choices.\n",
    "- **Problem Solving**: Identifies root causes and optimizes processes.\n",
    "- **Performance Evaluation**: Measures success and evaluates KPIs.\n",
    "- **Risk Management**: Identifies and mitigates potential risks.\n",
    "- **Gathering Insights**: Drives innovation and strategic actions.\n",
    "\n",
    "This tree structure organizes data analysis and visualization concepts systematically, making it easier to understand how to approach data analysis, choose the right techniques, and select appropriate visualizations based on your data and goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2- Structured Flow for Implementing Data Analysis\n",
    "\n",
    "The process of implementing data analysis can be broken down into clear steps, starting from problem identification and leading up to decision-making based on the analysis. Below is a structured approach for implementing data analysis:\n",
    "\n",
    "#### Step 1: Define the Objective\n",
    "- **Action**: Clearly identify the question or problem you want to solve with data.\n",
    "- **Example**: \"What are the key factors influencing customer churn?\"\n",
    "\n",
    "#### Step 2: Collect and Prepare the Data\n",
    "- **Action**: Gather data relevant to the analysis.  \n",
    "  Ensure data quality (handle missing values, duplicates, or irrelevant information).\n",
    "- **Example**: Collect customer demographic data, transaction history, and support interaction logs.\n",
    "\n",
    "#### Step 3: Identify the Type of Data\n",
    "- **Action**: Classify your data into Quantitative or Qualitative:\n",
    "  - **Quantitative**: Numerical, measurable (e.g., age, sales, etc.).\n",
    "  - **Qualitative**: Descriptive, non-numerical (e.g., customer feedback, reviews).\n",
    "- **Example**: Customer age (quantitative), customer feedback (qualitative).\n",
    "\n",
    "#### Step 4: Choose the Type of Data Analysis\n",
    "- **Action**: Select the type of analysis based on your objective:\n",
    "  - **Univariate Analysis** (for one variable): Summarize basic statistics (mean, median, etc.).\n",
    "  - **Bivariate Analysis** (for two variables): Identify relationships (e.g., correlation).\n",
    "  - **Multivariate Analysis** (for multiple variables): Explore interactions among many variables.\n",
    "- **Example**: Analyze customer churn using Bivariate analysis (e.g., age vs. churn, tenure vs. churn).\n",
    "\n",
    "#### Step 5: Select the Appropriate Data Analysis Technique\n",
    "- **Action**: Based on the data type and analysis, choose the most suitable technique:\n",
    "  - **Descriptive**: Summarize key statistics.\n",
    "  - **Diagnostic**: Identify reasons or relationships.\n",
    "  - **Predictive**: Forecast future behavior or outcomes.\n",
    "  - **Prescriptive**: Provide recommendations for optimal action.\n",
    "- **Example**: For customer churn prediction, Predictive analysis using a regression model or machine learning.\n",
    "\n",
    "#### Step 6: Visualize the Data\n",
    "- **Action**: Choose appropriate visualizations based on your analysis type:\n",
    "  - **Distribution**: Histograms, Boxplots.\n",
    "  - **Correlation**: Scatter Plots, Heatmaps.\n",
    "  - **Ranking**: Bar Charts, Dot Plots.\n",
    "  - **Part-of-Whole**: Pie Charts, Stacked Bars.\n",
    "  - **Evolution**: Line Charts, Area Charts.\n",
    "  - **Map**: Geospatial Visualization.\n",
    "  - **Networks**: Network Graphs.\n",
    "- **Example**: Use scatter plots to visualize the relationship between age and churn rate, or line charts to track churn over time.\n",
    "\n",
    "#### Step 7: Apply the Chosen Analysis Method\n",
    "- **Action**: Apply the selected technique using appropriate tools:\n",
    "  - **Statistical Methods** (e.g., hypothesis testing, regression).\n",
    "  - **Data Mining** (e.g., clustering, classification).\n",
    "  - **Text Mining** (e.g., sentiment analysis).\n",
    "  - **Time Series** (e.g., forecasting, decomposition).\n",
    "- **Example**: Apply predictive analysis using regression or machine learning algorithms to predict churn.\n",
    "\n",
    "#### Step 8: Interpret the Results\n",
    "- **Action**: Review the output of your analysis and interpret the results:\n",
    "  - What patterns, trends, or correlations have emerged?\n",
    "  - What are the key insights, and what do they mean in the context of your problem?\n",
    "- **Example**: Identify that age and customer tenure are strong predictors of churn, with younger customers and those with shorter tenure more likely to leave.\n",
    "\n",
    "#### Step 9: Make Data-Driven Decisions\n",
    "- **Action**: Use the insights to make informed decisions or recommendations:\n",
    "  - Suggest improvements, strategies, or actions based on the analysis.\n",
    "- **Example**: Develop a retention strategy focusing on younger customers and offering incentives to long-term customers to reduce churn.\n",
    "\n",
    "#### Step 10: Communicate the Findings\n",
    "- **Action**: Present your findings clearly to stakeholders:\n",
    "  - Use visualizations to highlight key points (charts, graphs).\n",
    "  - Provide a clear narrative of the insights and actionable recommendations.\n",
    "- **Example**: Present a report and dashboard to executives, showcasing the key churn drivers, visualizing trends, and suggesting actions for retention.\n",
    "\n",
    "#### Step 11: Take Action & Monitor\n",
    "- **Action**: Implement the suggested actions and monitor their effectiveness over time:\n",
    "  - Use **Prescriptive Analysis** to optimize strategies and actions.\n",
    "  - Monitor outcomes to see if the decisions lead to improved results.\n",
    "- **Example**: After implementing retention strategies, monitor churn rates and evaluate the success of your interventions.\n",
    "\n",
    "#### Step 12: Iteration and Refinement\n",
    "- **Action**: Data analysis is an iterative process:\n",
    "  - As new data comes in, revisit the analysis to refine insights or improve predictions.\n",
    "- **Example**: Track churn metrics monthly, refining your predictive model with new customer data.\n",
    "\n",
    "#### Tools & Resources Needed:\n",
    "- **Basic Tools**: Excel, Google Sheets (for simple analysis).\n",
    "- **Advanced Tools**: R, Python (for machine learning and deep analysis).\n",
    "- **Visualization Tools**: Tableau, Power BI (for interactive reports and dashboards).\n",
    "- **Cloud Analytics**: AWS, Azure (for large-scale data processing).\n",
    "\n",
    "### Summary of the Flow:\n",
    "- **Define the objective ‚Üí Collect and prepare data ‚Üí Identify data types ‚Üí Choose analysis type ‚Üí Select analysis technique ‚Üí Visualize the data ‚Üí Apply analysis methods ‚Üí Interpret the results ‚Üí Make decisions ‚Üí Communicate findings ‚Üí Take action ‚Üí Refine and iterate**.\n",
    "\n",
    "This flow ensures a systematic approach to data analysis and visualization, enabling you to extract meaningful insights, make informed decisions, and optimize outcomes for your organization or research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3- Plots explanation and their use case\n",
    "`Line Plot: Use plt.plot() to visualize continuous data.\n",
    "Scatter Plot: Use plt.scatter() to show relationships between two variables.\n",
    "Bar Plot: Use plt.bar() for categorical data.\n",
    "Histogram: Use plt.hist() to show the distribution of data.\n",
    "Heatmap: Use sns.heatmap() for 2D data with color coding.\n",
    "Pair Plot: Use sns.pairplot() for visualizing pairwise relationships.\n",
    "Joint Plot: Use sns.jointplot() for examining the relationship between two variables.\n",
    "Customizing Plots: Add annotations using plt.annotate(), change styles using plt.style.use(), and apply themes using sns.set_theme().\n",
    "Saving Plots: Use plt.savefig() to save plots in different formats like .png, .svg.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### libraries to Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a theme for Seaborn\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Sample Data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "categories = ['A', 'B', 'C', 'D']\n",
    "values = [5, 7, 3, 8]\n",
    "data = np.random.randn(1000)\n",
    "df = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plot\n",
    "A line plot is used to display data points in a continuous line, typically for time-series or continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Line Plot with Customization (Annotations, Styles, and Themes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, color='b', label='sin(x)')\n",
    "plt.title('Line Plot with Customizations')\n",
    "plt.xlabel('X axis')\n",
    "plt.ylabel('Y axis')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Add annotation\n",
    "plt.annotate('Peak', xy=(np.pi/2, 1), xytext=(3, 0.5),\n",
    "             arrowprops=dict(facecolor='red', arrowstyle='->'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "A scatter plot is used to show the relationship between two variables, where each point represents one observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scatter Plot with Customization (Annotations, Styles, and Themes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x[:50], y[:50], color='green', label='Data points', alpha=0.6)\n",
    "plt.title('Scatter Plot with Customizations')\n",
    "plt.xlabel('X axis')\n",
    "plt.ylabel('Y axis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plot\n",
    "A bar plot is used to represent categorical data with rectangular bars where the length of each bar represents a category's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bar Plot with Customization (Annotations, Styles, and Themes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(categories, values, color='orange')\n",
    "plt.title('Bar Plot with Customizations')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Add annotation\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.2, str(v), ha='center', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "A histogram is used to represent the distribution of a dataset by dividing the data into intervals or bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Histogram with Customization (Annotations, Styles, and Themes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(data, bins=30, color='purple', edgecolor='black')\n",
    "plt.title('Histogram with Customizations')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap\n",
    "A heatmap is a 2D representation of data where individual values are represented by colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Heatmap with Customization (Annotations, Styles, and Themes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "heatmap_data = np.random.rand(10, 12)\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', cbar=True)\n",
    "plt.title('Heatmap with Customizations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Plot\n",
    "A pair plot shows pairwise relationships in a dataset. It plots each variable against every other variable in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Pair Plot with Customization (Annotations, Styles, and Themes)\n",
    "sns.pairplot(df, hue='species', palette='Set2', markers='o')\n",
    "plt.title('Pair Plot with Customizations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Plot\n",
    "A joint plot is used to show the relationship between two variables along with their marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Joint Plot as a PNG file\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.jointplot(x='sepal_length', y='sepal_width', data=df, kind='scatter', color='blue')\n",
    "plt.suptitle('Joint Plot with Customizations', fontsize=16)\n",
    "plt.subplots_adjust(top=0.95)  # Adjust title to fit within the plot area\n",
    "plt.savefig('joint_plot_customized.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving plot as png or svg\n",
    "if we want to save any of these plots as images (e.g., PNG or SVG), \n",
    "we can simply use plt.savefig() right before plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('line_plot_customized.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Real World Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\\\Programming\\\\internship\\\\week3assignment\\\\merch_sales.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the dataset\n",
    "Summarize the dataset to understand its structure, missing values, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Summary:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the Dataset:\n",
    "**Handle missing values.**\n",
    "**Remove duplicates or irrelevant data if necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].fillna('No Review')  # Fill missing reviews with 'No Review'\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "# Perform exploratory data analysis (EDA)\n",
    "print(\"\\nEDA:\")\n",
    "print(\"Unique Product Categories:\", df['Product Category'].unique())\n",
    "print(\"Unique Buyer Genders:\", df['Buyer Gender'].unique())\n",
    "print(\"Unique Order Locations:\", df['Order Location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Data:\n",
    "Perform exploratory data analysis (EDA) to derive insights.\n",
    "Generate advanced computations using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate advanced computations using NumPy\n",
    "average_sales_price = np.mean(df['Sales Price'])\n",
    "print(\"\\nAverage Sales Price:\", average_sales_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Visualizations:\n",
    "Generate insightful visualizations using Matplotlib.\n",
    "Document findings and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Visualizations\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Distribution of Sales Price\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(df['Sales Price'], bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Sales Price')\n",
    "plt.xlabel('Sales Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 2. Sales by Product Category\n",
    "plt.subplot(2, 2, 2)\n",
    "df['Product Category'].value_counts().plot(kind='bar', color='green')\n",
    "plt.title('Sales by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Number of Sales')\n",
    "\n",
    "# 3. Sales by Buyer Gender\n",
    "plt.subplot(2, 2, 3)\n",
    "df['Buyer Gender'].value_counts().plot(kind='bar', color='purple')\n",
    "plt.title('Sales by Buyer Gender')\n",
    "plt.xlabel('Buyer Gender')\n",
    "plt.ylabel('Number of Sales')\n",
    "\n",
    "# 4. Average Rating by Product Category\n",
    "plt.subplot(2, 2, 4)\n",
    "df.groupby('Product Category')['Rating'].mean().plot(kind='bar', color='orange')\n",
    "plt.title('Average Rating by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Average Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dashboard:\n",
    "Combine multiple visualizations into a dashboard using Matplotlib's subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dashboard\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Distribution of Sales Price\n",
    "axs[0, 0].hist(df['Sales Price'], bins=30, color='blue', edgecolor='black')\n",
    "axs[0, 0].set_title('Distribution of Sales Price')\n",
    "axs[0, 0].set_xlabel('Sales Price')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Sales by Product Category\n",
    "df['Product Category'].value_counts().plot(kind='bar', color='green', ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Sales by Product Category')\n",
    "axs[0, 1].set_xlabel('Product Category')\n",
    "axs[0, 1].set_ylabel('Number of Sales')\n",
    "\n",
    "# 3. Sales by Buyer Gender\n",
    "df['Buyer Gender'].value_counts().plot(kind='bar', color='purple', ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Sales by Buyer Gender')\n",
    "axs[1, 0].set_xlabel('Buyer Gender')\n",
    "axs[1, 0].set_ylabel('Number of Sales')\n",
    "\n",
    "# 4. Average Rating by Product Category\n",
    "df.groupby('Product Category')['Rating'].mean().plot(kind='bar', color='orange', ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Average Rating by Product Category')\n",
    "axs[1, 1].set_xlabel('Product Category')\n",
    "axs[1, 1].set_ylabel('Average Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Documentation of the Above Real World Project ,for Sales Data Analysis Workflow with Each Steps Implementatio\n",
    "---\n",
    "#### 1. Define the Objective\n",
    "Analyzed the sales data to identify key trends and insights such as high-performing product categories, customer demographics, and sales patterns.\n",
    "---\n",
    "#### 2. Collect and Prepare Data\n",
    "- The dataset is already loaded into a DataFrame named `df`.\n",
    "- **Columns and Data Types:**\n",
    "Order ID int64 Order Date object Product ID object Product Category object Buyer Gender object Buyer Age int64 Order Location object International Shipping object Sales Price int64 Shipping Charges int64 Sales per Unit int64 Quantity int64 Total Sales int64 Rating int64 Review object\n",
    "---\n",
    "#### 3. Identify Data Types\n",
    "- The dataset contains:\n",
    "- **Categorical Variables:** `Product Category`, `Buyer Gender`, `Order Location`, `International Shipping`, `Review`.\n",
    "- **Numerical Variables:** `Sales Price`, `Shipping Charges`, `Sales per Unit`, `Quantity`, `Total Sales`, `Rating`, `Buyer Age`.\n",
    "---\n",
    "#### 4. Choose Analysis Type\n",
    "- **Univariate Analysis:** To explore distributions of individual variables like `Sales Price`, `Rating`, and `Buyer Gender`.\n",
    "- **Bivariate Analysis:** To analyze relationships between variables, e.g., `Product Category` vs. `Sales Price` or `Rating` vs. `Total Sales`.\n",
    "---\n",
    "#### 5. Select Analysis Techniques\n",
    "- **Descriptive Statistics:** Mean, median, standard deviation of numerical columns.\n",
    "- **Visualizations:** Histograms, bar charts, and correlation heatmaps.\n",
    "- **Correlation Analysis:** To measure relationships between numerical variables.\n",
    "---\n",
    "#### 6. Visualize the Data\n",
    "`plt.figure(figsize=(14, 10))`\n",
    "**Distribution of Sales Price**\n",
    "`plt.subplot(2, 2, 1)\n",
    "plt.hist(df['Sales Price'], bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Sales Price')\n",
    "plt.xlabel('Sales Price')\n",
    "plt.ylabel('Frequency')`\n",
    "\n",
    "**Sales by Product Category**\n",
    "`plt.subplot(2, 2, 2)\n",
    "df['Product Category'].value_counts().plot(kind='bar', color='green')\n",
    "plt.title('Sales by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Number of Sales')`\n",
    "\n",
    "**Sales by Buyer Gender**\n",
    "`plt.subplot(2, 2, 3)\n",
    "df['Buyer Gender'].value_counts().plot(kind='bar', color='purple')\n",
    "plt.title('Sales by Buyer Gender')\n",
    "plt.xlabel('Buyer Gender')\n",
    "plt.ylabel('Number of Sales')`\n",
    "\n",
    "**Average Rating by Product Category**\n",
    "`plt.subplot(2, 2, 4)\n",
    "df.groupby('Product Category')['Rating'].mean().plot(kind='bar', color='orange')\n",
    "plt.title('Average Rating by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.tight_layout()\n",
    "plt.show()`\n",
    "\n",
    "#### 7. Apply Analysis Methods\n",
    "**Descriptive Statistics:** To summarize numerical columns (Sales Price, Total Sales, Rating).\n",
    "**Correlation Analysis:** Calculating the correlation matrix for numerical variables.\n",
    "`correlation_matrix = df.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)`\n",
    "\n",
    "#### 8. Interpret the Results\n",
    "**Distribution of Sales Price:** Understand the frequency of different price ranges.\n",
    "**High-Selling Product Categories:** From the bar chart, identify which product categories have the most sales.\n",
    "**Buyer Demographics:** Analyze the gender distribution to see the major contributing group.\n",
    "**Correlation Insights:** Identify relationships, e.g., between Sales Price, Quantity, and Total Sales.\n",
    "\n",
    "#### Implementation of Steps 4 to 8\n",
    "**Univariate Analysis (Step 4):**\n",
    "\n",
    "Histogram of Sales Price to visualize distribution.\n",
    "Count plot for Product Category and Buyer Gender.\n",
    "**Bivariate Analysis (Step 5):**\n",
    "Bar chart to analyze the relationship between Product Category and average Rating.\n",
    "**Analysis Techniques (Step 6):**\n",
    "Use visualizations for patterns and correlations.\n",
    "Generate a correlation matrix to assess numerical relationships.\n",
    "**Insights and Results (Step 7 and 8):**\n",
    "Identify categories with high average ratings and significant correlations, e.g., if higher Quantity leads to lower Sales Price per unit.\n",
    "\n",
    "#### 9. Make Decisions\n",
    "**Example decisions:**\n",
    "Focus marketing efforts on high-selling product categories.\n",
    "Target underrepresented buyer demographics for growth opportunities.\n",
    "Optimize pricing strategies based on insights from the correlation matrix.\n",
    "\n",
    "#### 10. Communicate Findings\n",
    "Create a summary report with visualizations and key takeaways.\n",
    "Share findings with stakeholders through charts and actionable insights.\n",
    "\n",
    "#### 11. Take Action\n",
    "Implement recommendations such as adjusting marketing budgets or improving customer service in top-performing categories.\n",
    "\n",
    "#### 12. Refine and Iterate\n",
    "Continuously monitor sales data and refine strategies based on updated insights and new patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Things to remember\n",
    "**Data distribution:** describes the range of values in a dataset and how often each value occurs. It can be used to understand the data's center, spread, shape, and modality.\n",
    "-***Data distribution can be represented graphically using histograms and box plots.***\n",
    "-Two types of data distribution: discrete and continuous. Discrete data has specific values, while continuous data can have an infinite number of values.\n",
    "**Data manipulation** is the process of changing or organizing data to make it more accessible, readable, and useful: \n",
    "**Purpose**\n",
    "Data manipulation is a key step in data analysis and processing. It helps prepare data for analysis, reporting, visualization, and other computational tasks. \n",
    "**Techniques**\n",
    "Data manipulation can involve a variety of operations, including filtering, sorting, aggregating, merging, and transforming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data can be categorised into three types:\n",
    "- **Missing Completely at Random (MCAR):** This means that the missing data\n",
    "is not in any way related to any other data in the dataset.\n",
    "- **Missing at Random (MAR):** This is when the missing data is somehow\n",
    "related to other data in the dataset. \n",
    "- **Missing Not at Random (MNAR):** This occurs when there is a direct relation\n",
    "to some missing data that the researchers haven‚Äôt measured.\n",
    "**Standardization** is useful when you want to center your data with a mean of 0 and a standard deviation of 1, especially when your algorithm assumes that the data is normally distributed (e.g., linear regression, PCA).\n",
    "## Methods to remove the Missing Data:\n",
    "- **distribution of data**: statistics using mean,max,etc\n",
    "- **imputation of KNN**:\n",
    "- **mputation with linear regression**\n",
    "This imputation technique utilises variables from the observed data to replace the\n",
    "missing values with predicted values from a regression model.\n",
    "\n",
    "## FEATURE SCALING: NORMALISATION AND STANDARDISATION\n",
    "Another common problem we encounter when trying to analyse data is having\n",
    "different units of measurement for a particular variable.\n",
    "### Methods for scaling\n",
    "- **Normalization**\n",
    "- **Standard Deviation**\n",
    "\n",
    "#### Example of Standardization\n",
    "#### Original Data:\n",
    "[10,12,14,16,18]\n",
    "**Mean (ùúá):** 10+12+14+16+1/5=14 <br>\n",
    "\n",
    "**Standard Deviation (ùúé):** Let's assume œÉ=2.83 for simplicity.<br>\n",
    "**Standardization:** <br>\n",
    "Using the formula <br>\n",
    "`ùëãstandardized=ùëã‚àíùúá/ùúé`<br>\n",
    "For X=10:\n",
    "ùëãstandardized=10‚àí14/2.83 =>‚àí1.41<br>\n",
    "for X=12:\n",
    "Xstandardized=12‚àí14/2.83=>‚àí2<br>\n",
    "Similar for all others aply same formula \n",
    "#### Standardized Data:\n",
    "[-1.41, -0.71, 0, 0.71, 1.41]\n",
    "\n",
    "Now, the data has a **mean of 0** and a **standard deviation of 1**.\n",
    "\n",
    "**Normalization** is useful when you need to scale data into a specific range (usually [0, 1]) and works well for distance-based algorithms and neural networks.<br>\n",
    "Formula:\n",
    "`Xnormalized= X‚àíXmin/Xmax‚àíXmin` <br>\n",
    "‚ÄãWhere:\n",
    "X is the original value,\n",
    "ùëãmin\n",
    "  is the minimum value in the dataset,\n",
    "ùëãmax‚Äã\n",
    "  is the maximum value in the dataset.\n",
    "\n",
    "`When to Use:`\n",
    "`When features have different units or scales` (e.g., height in cm vs. weight in kg).\n",
    "`Commonly used when applying algorithms that rely on distance metrics`, such as K-Nearest Neighbors (KNN) or Support Vector Machines (SVM).\n",
    "`In neural networks where the activation functions` (like Sigmoid) expect values between 0 and 1. <br>\n",
    "**But how do we know when**\n",
    "`to use which one? Quite simply, you must standardise the data when it follows a Gaussian distribution. If not, normalise the data.`\n",
    "\n",
    "Normalization and Standardization (Standard Deviation) are two common techniques used in data preprocessing to make datasets more comparable and improve machine learning model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Explore Data Analysis)\n",
    "When you're a beginner and find yourself stuck during the Exploratory Data Analysis (EDA) of a dataset, it's completely normal. EDA is a crucial step in data science that involves summarizing, visualizing, and interpreting the data to uncover patterns, anomalies, and insights. Here are some steps and tips to guide you through the process:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   - **Read the Data**: Use tools like pandas in Python to load the dataset (`pd.read_csv()`, `pd.read_excel()`, etc.).\n",
    "   - **Check the Structure**: Use `df.info()` in pandas to get an overview of the data types, missing values, and the number of entries for each column.\n",
    "   - **View the Data**: Use `df.head()` and `df.tail()` to see the first and last few rows of the dataset.\n",
    "\n",
    "2. **Clean the Data**:\n",
    "   - **Handle Missing Values**: Decide whether to fill them in, drop them, or use imputation techniques.\n",
    "   - **Remove Duplicates**: Use `df.drop_duplicates()` to ensure you're working with unique data points.\n",
    "   - **Correct Data Types**: Convert columns to the appropriate data types using `df['column'].astype()`.\n",
    "\n",
    "3. **Summarize the Data**:\n",
    "   - **Descriptive Statistics**: Use `df.describe()` to get summary statistics for numerical columns.\n",
    "   - **Categorical Data Summary**: Use `df['category_column'].value_counts()` to see the distribution of categorical variables.\n",
    "\n",
    "4. **Visualize the Data**:\n",
    "   - **Histograms**: Plot histograms for numerical columns to understand their distribution.\n",
    "   - **Box Plots**: Use box plots to identify outliers and the spread of the data.\n",
    "   - **Bar Charts/Pie Charts**: Visualize the frequency or proportion of categorical data.\n",
    "   - **Scatter Plots**: Plot scatter plots to explore relationships between two numerical variables.\n",
    "\n",
    "5. **Ask Questions and Formulate Hypotheses**:\n",
    "   - Think about what questions you want to answer with the data.\n",
    "   - Formulate hypotheses based on your initial observations and domain knowledge.\n",
    "\n",
    "6. **Dig Deeper**:\n",
    "   - **Correlation Analysis**: Use `df.corr()` to see how numerical variables are related to each other.\n",
    "   - **Grouping and Aggregation**: Use `groupby()` and aggregation functions to summarize data by categories.\n",
    "   - **Feature Engineering**: Create new features from existing ones if they might be useful for your analysis.\n",
    "\n",
    "7. **Document Your Findings**:\n",
    "   - Keep a notebook or document where you record your observations, hypotheses, and the steps you took during EDA.\n",
    "   - This will help you stay organized and communicate your findings effectively.\n",
    "\n",
    "8. **Iterate**:\n",
    "   - EDA is often an iterative process. As you uncover new insights, you may need to go back and refine your approach or explore new angles.\n",
    "\n",
    "9. **Seek Help**:\n",
    "   - Don't hesitate to ask for help from more experienced colleagues, online forums, or tutorials.\n",
    "   - The data science community is generally very helpful and supportive.\n",
    "\n",
    "Remember, the goal of EDA is to gain a deep understanding of your data so that you can make informed decisions about how to proceed with your analysis or modeling. Take your time, be methodical, and don't be afraid to explore different approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
